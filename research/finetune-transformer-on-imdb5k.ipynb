{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune a Transformer-based architecture on the IMDB Movie Reviews dataset for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies, save requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /opt/conda/lib/python3.6/site-packages (1.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.6/site-packages (from torch) (1.16.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas tqdm\n",
    "!pip install -U torch \n",
    "!pip install -q pytorch_transformers pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "tqdm\n",
    "torch==1.1.0\n",
    "pytorch_transformers\n",
    "pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# text and label column names\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# path to data \n",
    "DATA_DIR = os.path.abspath('./data')\n",
    "\n",
    "# path to IMDB\n",
    "IMDB_DIR = os.path.join(DATA_DIR, \"imdb5k\")\n",
    "\n",
    "# url to dataset\n",
    "IMDB_URL = \"https://github.com/ben0it8/transformer-finetuning/raw/master/imdb5k.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     3,
     33
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "\n",
    "def download_url(url:str, dest:str, overwrite:bool=True, show_progress=True, \n",
    "                 chunk_size=1024*1024, timeout=4, retries=5)->None:\n",
    "    \"Download `url` to `dest` unless it exists and not `overwrite`.\"\n",
    "    \n",
    "    dest = os.path.join(dest, os.path.basename(url))\n",
    "    if os.path.exists(dest) and not overwrite: \n",
    "        print(f\"File {dest} already exists!\")\n",
    "        return dest\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.mount('http://',requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    u = s.get(url, stream=True, timeout=timeout)\n",
    "    try: file_size = int(u.headers[\"Content-Length\"])\n",
    "    except: show_progress = False\n",
    "    print(f\"Downloading {url}\")\n",
    "    with open(dest, 'wb') as f:\n",
    "        nbytes = 0\n",
    "        if show_progress: \n",
    "            pbar = tqdm(range(file_size), leave=False)\n",
    "        try:\n",
    "            for chunk in u.iter_content(chunk_size=chunk_size):\n",
    "                nbytes += len(chunk)\n",
    "                if show_progress: pbar.update(nbytes)\n",
    "                f.write(chunk)\n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Download failed after {retries} retries.\")\n",
    "            import sys;sys.exit(1)\n",
    "        finally:\n",
    "            return dest\n",
    "        \n",
    "def untar(file_path, dest:str):\n",
    "    \"Untar `file_path` to `dest`\"\n",
    "    print(f\"Untar {os.path.basename(file_path)} to {dest}\")\n",
    "    with tarfile.open(file_path) as tf:\n",
    "        tf.extractall(path=str(dest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ben0it8/transformer-finetuning/raw/master/imdb5k.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba3824c995d44a19023567768cf813a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5382327), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untar imdb5k.tar.gz to /Users/d069049/Develop/transformer-finetuning/data\n"
     ]
    }
   ],
   "source": [
    "# download imdb dataset\n",
    "file_path = download_url(IMDB_URL, '/tmp', overwrite=True)\n",
    "\n",
    "# untar imdb dataset to DATA_DIR\n",
    "untar(file_path, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28928\r\n",
      "-rw-r--r--  1 d069049  staff   6.3M Jul 17 18:01 imdb5k_test.csv\r\n",
      "-rw-r--r--  1 d069049  staff   6.3M Jul 17 18:01 imdb5k_train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh $IMDB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_html(raw: str):\n",
    "    \"remove html tags and whitespaces\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, '  ', raw)\n",
    "    return re.sub(' +', ' ', clean)\n",
    "\n",
    "def read_imdb(data_dir, max_lengths={\"train\": None, \"test\": None}):\n",
    "    datasets = {}\n",
    "    for t in [\"train\", \"test\"]:\n",
    "        df = pd.read_csv(os.path.join(data_dir, f\"imdb5k_{t}.csv\"))\n",
    "        if max_lengths.get(t) is not None:\n",
    "            df = df.sample(n=max_lengths.get(t))\n",
    "            df[TEXT_COL] = df[TEXT_COL].apply(lambda t: clean_html(t))\n",
    "        datasets[t] = df\n",
    "    return datasets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data, 5000-5000 each\n",
    "datasets = read_imdb(IMDB_DIR)\n",
    "\n",
    "# list of labels\n",
    "labels = list(set(datasets[\"train\"][LABEL_COL].tolist()))\n",
    "\n",
    "# labels to integers mapping\n",
    "label2int = {label: i for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>neg</td>\n",
       "      <td>If this is the first of the \"Nemesis\" films th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>pos</td>\n",
       "      <td>This film is stunningly beautiful. Goldsworthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>neg</td>\n",
       "      <td>Basically, \"Caprica\" is the Cylon origin story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>pos</td>\n",
       "      <td>I saw this film a few years ago and I got to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>pos</td>\n",
       "      <td>RUMORS is a memorable entry in the wartime ser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "2172   neg  If this is the first of the \"Nemesis\" films th...\n",
       "1200   pos  This film is stunningly beautiful. Goldsworthy...\n",
       "4463   neg  Basically, \"Caprica\" is the Cylon origin story...\n",
       "1030   pos  I saw this film a few years ago and I got to s...\n",
       "956    pos  RUMORS is a memorable entry in the wartime ser..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "NUM_MAX_POSITIONS = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class TextProcessor:\n",
    "    \n",
    "    # special tokens for classification and padding\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    \n",
    "    def __init__(self, tokenizer, label2id: dict, num_max_positions:int=512):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.num_labels = len(label2id)\n",
    "        self.num_max_positions = num_max_positions\n",
    "        \n",
    "    \n",
    "    def process_example(self, example: Tuple[str, str]):\n",
    "        \"Convert text (example[0]) to sequence of IDs and label (example[1] to integer\"\n",
    "        assert len(example) == 2\n",
    "        label, text = example[0], example[1]\n",
    "        assert isinstance(text, str)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(tokens) >= self.num_max_positions:\n",
    "            tokens = tokens[:self.num_max_positions-1] \n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]]\n",
    "        # pad if too short\n",
    "        else:\n",
    "            pad = [self.tokenizer.vocab[self.PAD]] * (self.num_max_positions-len(tokens)-1)\n",
    "            ids =  self.tokenizer.convert_tokens_to_ids(tokens) + [self.tokenizer.vocab[self.CLS]] + pad\n",
    "        \n",
    "        return ids, self.label2id[label]\n",
    "    \n",
    "# download the 'bert-base-cased' tokenizer\n",
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "# initialize a TextProcessor\n",
    "processor = TextProcessor(tokenizer, label2int, num_max_positions=NUM_MAX_POSITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningConfig(num_classes=2, dropout=0.1, init_range=0.02, batch_size=32, lr=6.5e-05, max_norm=1.0, n_epochs=2, n_warmup=10, valid_pct=0.1, gradient_acc_steps=1, device=device(type='cpu'), log_dir='./logs/', dataset_cache='./cache/dataset_cache.bin')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "\n",
    "LOG_DIR = \"./logs/\"\n",
    "CACHE_DIR = \"./cache/\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "FineTuningConfig = namedtuple('FineTuningConfig',\n",
    "      field_names=\"num_classes, dropout, init_range, batch_size, lr, max_norm, n_epochs,\"\n",
    "                  \"n_warmup, valid_pct, gradient_acc_steps, device, log_dir, dataset_cache\")\n",
    "\n",
    "finetuning_config = FineTuningConfig(\n",
    "                2, 0.1, 0.02, BATCH_SIZE, 6.5e-5, 1.0, 2,\n",
    "                10, 0.1, 1, device, LOG_DIR, \n",
    "                CACHE_DIR+'dataset_cache.bin')\n",
    "\n",
    "finetuning_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df: pd.DataFrame, processor: TextProcessor, batch_size:int=32, shuffle:bool=False, valid_pct:float=None, \n",
    "                   text_col:str=\"text\", label_col:str=\"label\"):\n",
    "    \"Process rows in `df` with `processor` and return a  DataLoader\"\n",
    "    \n",
    "    features, labels = [], [] \n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        ids, lbl = processor.process_example((row[LABEL_COL], row[TEXT_COL]))\n",
    "        features += [ids]\n",
    "        labels += [lbl]\n",
    "    \n",
    "    dataset = TensorDataset(\n",
    "                    torch.tensor(features, dtype=torch.long), \n",
    "                    torch.tensor(labels, dtype=torch.long))\n",
    "    \n",
    "    if valid_pct is not None:\n",
    "        valid_size = int(valid_pct * len(df))\n",
    "        train_size = len(df) - valid_size\n",
    "        valid_dataset, train_dataset = random_split(dataset, [valid_size, train_size])\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    \n",
    "        return train_loader, valid_loader\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2454b587c961450ea63108034a37ee50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c982aa0ebbd4bdfa5cccb78a7279e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create train and valid sets by splitting\n",
    "train_dl, valid_dl = create_dataloaders(datasets[\"train\"], processor, \n",
    "                                    batch_size=finetuning_config.batch_size, \n",
    "                                    valid_pct=finetuning_config.valid_pct)\n",
    "\n",
    "test_dl = create_dataloaders(datasets[\"test\"], processor, \n",
    "                             batch_size=finetuning_config.batch_size, \n",
    "                             valid_pct=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerWithClfHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     6,
     52
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_num_params(model):\n",
    "    mp = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum(np.prod(p.size()) for p in mp)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_embeddings, num_max_positions, num_heads, num_layers, dropout, causal):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.tokens_embeddings = nn.Embedding(num_embeddings, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(num_max_positions, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.attentions.append(nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout))\n",
    "            self.feed_forwards.append(nn.Sequential(nn.Linear(embed_dim, hidden_dim),\n",
    "                                                    nn.ReLU(),\n",
    "                                                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.layer_norms_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.layer_norms_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        \"\"\" x has shape [seq length, batch], padding_mask has shape [batch, seq length] \"\"\"\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)\n",
    "        h = self.tokens_embeddings(x)\n",
    "        h = h + self.position_embeddings(positions).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        attn_mask = None\n",
    "        if self.causal:\n",
    "            attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)\n",
    "            attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,\n",
    "                                                                       self.layer_norms_2, self.feed_forwards):\n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "\n",
    "            h = layer_norm_2(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h\n",
    "        return h\n",
    "\n",
    "\n",
    "class TransformerWithClfHead(nn.Module):\n",
    "    \"Adopted from https://github.com/huggingface/naacl_transfer_learning_tutorial\"\n",
    "    def __init__(self, config, fine_tuning_config):\n",
    "        super().__init__()\n",
    "        self.config = fine_tuning_config\n",
    "        self.transformer = Transformer(config.embed_dim, config.hidden_dim, config.num_embeddings,\n",
    "                                       config.num_max_positions, config.num_heads, config.num_layers,\n",
    "                                       fine_tuning_config.dropout, causal=not config.mlm)\n",
    "        \n",
    "        self.classification_head = nn.Linear(config.embed_dim, fine_tuning_config.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.init_range)\n",
    "        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, clf_tokens_mask, clf_labels=None, padding_mask=None):\n",
    "        hidden_states = self.transformer(x, padding_mask)\n",
    "\n",
    "        clf_tokens_states = (hidden_states * clf_tokens_mask.unsqueeze(-1).float()).sum(dim=0)\n",
    "        clf_logits = self.classification_head(clf_tokens_states)\n",
    "\n",
    "        if clf_labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "            loss = loss_fct(clf_logits.view(-1, clf_logits.size(-1)), clf_labels.view(-1))\n",
    "            return clf_logits, loss\n",
    "        return clf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters discarded from the pretrained model: ['lm_head.weight']\n",
      "Parameters added in the model: ['classification_head.weight', 'classification_head.bias']\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import cached_path\n",
    "\n",
    "# download pre-trained model and config\n",
    "state_dict = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                    \"naacl-2019-tutorial/model_checkpoint.pth\"), map_location='cpu')\n",
    "\n",
    "config = torch.load(cached_path(\"https://s3.amazonaws.com/models.huggingface.co/\"\n",
    "                                        \"naacl-2019-tutorial/model_training_args.bin\"))\n",
    "\n",
    "# init model: Transformer base + classifier head\n",
    "model = TransformerWithClfHead(config=config, fine_tuning_config=finetuning_config).to(finetuning_config.device)\n",
    "\n",
    "incompatible_keys = model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Parameters discarded from the pretrained model: {incompatible_keys.unexpected_keys}\")\n",
    "print(f\"Parameters added in the model: {incompatible_keys.missing_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50397182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare fine-tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     38
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import RunningAverage, Accuracy \n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import CosineAnnealingScheduler, PiecewiseLinear, create_lr_scheduler_with_warmup, ProgressBar\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers.optimization import AdamW\n",
    "\n",
    "# Bert optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=finetuning_config.lr, correct_bias=False) \n",
    "\n",
    "def update(engine, batch):\n",
    "    \"update function for training\"\n",
    "    model.train()\n",
    "    inputs, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "    inputs = inputs.transpose(0, 1).contiguous() # [S, B]\n",
    "    _, loss = model(inputs, \n",
    "                    clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]), \n",
    "                    clf_labels=labels)\n",
    "    loss = loss / finetuning_config.gradient_acc_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), finetuning_config.max_norm)\n",
    "    if engine.state.iteration % finetuning_config.gradient_acc_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return loss.item()\n",
    "\n",
    "def inference(engine, batch):\n",
    "    \"update function for evaluation\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch, labels = (t.to(finetuning_config.device) for t in batch)\n",
    "        inputs = batch.transpose(0, 1).contiguous()\n",
    "        logits = model(inputs,\n",
    "                       clf_tokens_mask = (inputs == tokenizer.vocab[processor.CLS]),\n",
    "                       padding_mask = (batch == tokenizer.vocab[processor.PAD]))\n",
    "    return logits, labels\n",
    "\n",
    "def predict(model, tokenizer, int2label, input=\"test\"):\n",
    "    \"predict `input` with `model`\"\n",
    "    tok = tokenizer.tokenize(input)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tok) + [tokenizer.vocab['[CLS]']]\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    tensor = tensor.to(device)\n",
    "    tensor = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor.transpose(0, 1).contiguous() # [S, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask = (tensor_in == tokenizer.vocab['[CLS]']),\n",
    "                   padding_mask = (tensor == tokenizer.vocab['[PAD]']))\n",
    "    val, _ = torch.max(logits, 0)\n",
    "    val = F.softmax(val, dim=0).detach().cpu().numpy()    \n",
    "    return {int2label[val.argmax()]: val.max(),\n",
    "            int2label[val.argmin()]: val.min()}\n",
    "trainer = Engine(update)\n",
    "evaluator = Engine(inference)\n",
    "\n",
    "# add metric to evaluator \n",
    "Accuracy().attach(evaluator, \"accuracy\")\n",
    "\n",
    "# add evaluator to trainer: eval on valid set after each epoch\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(valid_dl)\n",
    "    print(f\"validation epoch: {engine.state.epoch} acc: {100*evaluator.state.metrics['accuracy']}\")\n",
    "          \n",
    "# lr schedule: linearly warm-up to lr and then to zero\n",
    "scheduler = PiecewiseLinear(optimizer, 'lr', [(0, 0.0), (finetuning_config.n_warmup, finetuning_config.lr),\n",
    "                                              (len(train_dl)*finetuning_config.n_epochs, 0.0)])\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "\n",
    "\n",
    "# add progressbar with loss\n",
    "RunningAverage(output_transform=lambda x: x).attach(trainer, \"loss\")\n",
    "ProgressBar(persist=True).attach(trainer, metric_names=['loss'])\n",
    "\n",
    "# save checkpoints and finetuning config\n",
    "checkpoint_handler = ModelCheckpoint(finetuning_config.log_dir, 'finetuning_checkpoint', \n",
    "                                     save_interval=1, require_empty=False)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpoint_handler, {'imdb_model': model})\n",
    "\n",
    "# save config to logdir\n",
    "torch.save(finetuning_config, os.path.join(finetuning_config.log_dir, 'fine_tuning_args.bin'))          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets fine-tune on imdb!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d342879fdf8a4ffd96a8a44818a68b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 1 acc: 84.2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea04dae93c9d4c148600b9727ceec029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation epoch: 2 acc: 86.4\n",
      "\n",
      "test results - acc: 89.800\n"
     ]
    }
   ],
   "source": [
    "# fit the model on train_dl\n",
    "trainer.run(train_dl, max_epochs=finetuning_config.n_epochs)\n",
    "\n",
    "# evaluate the model on test_dl\n",
    "evaluator.run(test_dl)\n",
    "print(f\"test results - acc: {100*evaluator.state.metrics['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 196912\r\n",
      "-rw-r--r--. 1 root root       318 Jul 17 09:56 fine_tuning_args.bin\r\n",
      "-rw-------. 1 root root 201630224 Jul 17 09:59 finetuning_checkpoint_imdb_model_2.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $finetuning_config.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "int2label = {i:label for label,i in label2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 0.9117301, 'neg': 0.08826993}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, tokenizer, int2label, input = \"I just love how the actors are playing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.9916163, 'pos': 0.008383713}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model, tokenizer, int2label, input = \"This movie is poorly directed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
